#!/usr/bin/env python3
#
# Copyright (c) 2025 by Gilbert Ramirez <gram@alumni.rice.edu>
#
# This reads the data in the TSV of country data, and generates
# a Gren file which provides a searchable database of these records.

import argparse
from collections import namedtuple
import unicodedata
import sys

Country = namedtuple("Country", ["name", "officialName", "capital", "population"])

# The country ID, and the nth word in the country name where
# the associated substring is found
CountryPosition = namedtuple("CountryPosition", ["cid", "nth"])

# The substring, and the nth word in the country name where
# it is found
SSPos = namedtuple("SSPos", ["text", "nth"])

class CountryDatabase:
    def __init__(s):
        s.nextId = 0

        # All Country objects
        s.countries = []

        # Key = lowercase string, Value = [ CountryPosition ]
        s.substringLocations = {}

    def add_country(s, country):
        countryId = s.nextId
        s.nextId += 1
        s.countries.append(country)
        s.add_substrings(countryId, country.name)

    def add_substrings(s, countryId, countryName):
        lower_name = countryName.lower()
        ascii_lower_name = remove_accents(lower_name)
        for sspos in get_sspos_for_all_words(ascii_lower_name):
            ssList = s.substringLocations.setdefault(sspos.text, list())
            cpos = CountryPosition(countryId, sspos.nth)
            ssList.append(cpos)

    def write_gren_file(s, gren_filename, gren_module):
        with open(gren_filename, "w") as fh:
            s.gren_header(fh, gren_module)
            print("\n", file=fh)
            s.gren_create_by_id_dict(fh)
            print("\n", file=fh)
            s.gren_create_substring_map(fh)
            print("\n", file=fh)
            s.gren_db(fh)

    def gren_header(s, fh, gren_module):
            print(f"module {gren_module} exposing (db)", file=fh)
            progName = sys.argv[0]
            print("\n-- This file is generated by {progName}. DO NOT EDIT MANUALLY", file=fh)
            print("", file=fh)
            print("import Dict exposing (Dict)", file=fh)
            print("import Country exposing (Country, CountryDatabase)", file=fh)

    def gren_db(s, fh):
        print("db : CountryDatabase", file=fh)
        print("db =", file=fh)
        print("    { byId = create_by_id_dict", file=fh)
        print("    , substringMap = create_substring_map", file=fh)
        print("    }", file=fh)

    def gren_create_by_id_dict(s, fh):
        print("create_by_id_dict : Dict Int Country", file=fh)
        print("create_by_id_dict = ", file=fh)
        print("    Dict.empty", file=fh)
        for cid, country in enumerate(s.countries):
            gren_country = s.format_country_for_gren(country)
            print(f"        |> Dict.set {cid} {gren_country}", file=fh)

    def format_country_for_gren(s, country):
        c = country
        return f'{{ name = "{c.name}", officialName = "{c.officialName}",' + \
            f' capital = "{c.capital}", population = {c.population} }}'

    def gren_create_substring_map(s, fh):
        print("create_substring_map : Dict String (Array Int)", file=fh)
        print("create_substring_map = ", file=fh)
        print("    Dict.empty", file=fh)
        substrings = sorted(s.substringLocations.keys())
        for ss in substrings:
            # Sort locations first by their position within
            # the string, and then if at the same position,
            # the alphaetical ordering of the country (which can
            # be ascertained by the country ID itself)
            locations = sorted(s.substringLocations[ss],
                           key = lambda cpos: 1000 * cpos.nth + cpos.cid)

            location_chunks = chunk_list(locations, 10)
            for i, chunk in enumerate(location_chunks):
                isFirst = i == 0
                isLast = i == len(location_chunks) -1

                # The positions were used for sorting. Now we just
                # want the country IDs, but still in that same
                # sorted order
                addedCids = set()
                sortedCids = []
                for cpos in chunk:
                    if cpos.cid in addedCids:
                        continue
                    else:
                        addedCids.add(cpos.cid)
                        sortedCids.append(cpos.cid)

                # Format the sorted country IDs for gren
                gren_cids = ", ".join([str(n) for n in sortedCids])

                if isFirst and isLast:
                    print(f'        |> Dict.set "{ss}" [ {gren_cids} ]', file=fh)
                elif isFirst:
                    print(f'        |> Dict.set "{ss}" [ {gren_cids},', file=fh)
                elif isLast:
                    print(f'                {gren_cids} ]', file=fh)
                else:
                    print(f'                {gren_cids},', file=fh)

# Thank you, Gemini
def chunk_list(long_list, max_size):
    """
    Breaks a long list into a list of smaller lists (chunks),
    each with a maximum size.

    Args:
        long_list: The input list to be chunked.
        max_size: The maximum number of items in each chunk. Defaults to 10.

    Returns:
        A list of lists, where each inner list is a chunk.
    """

    # We use a list comprehension with slicing to create the chunks.
    # The range step is set to max_size.
    return [
        long_list[i:i + max_size]
        for i in range(0, len(long_list), max_size)
    ]

# Remove accents and other diactrics from Latin-type code points
# into regular ASCII
# Thank you, Gemini
def remove_accents(input_string):
    nfd_form = unicodedata.normalize('NFD', input_string)
    ascii_string = nfd_form.encode('ascii', 'ignore').decode('utf-8')
    return ascii_string

def get_sspos_for_all_words(text):
    words = text.split()
    sspositions = []
    for n, word in enumerate(words):
        for ss in get_all_initial_substrings(word):
            sspositions.append( SSPos(ss, n) )
    return sspositions

def get_all_initial_substrings(text):
    substring_data = []
    n = len(text)

    for i in range(1, n+1):
        substring = text[0:i]
        substring_data.append(substring)

    return substring_data

def build_database(tsv_filename):
    db = CountryDatabase()

    with open(tsv_filename) as fh:
        lineno = 0
        for line in fh.readlines():
            lineno += 1
            # Skip the header
            if lineno == 1 and line.startswith("Country"):
                continue
            # Remove the trailing newline
            line = line.rstrip()
            fields = line.split("\t")
            assert len(fields) == 4, f"Line {lineno} has {len(fields)} fields: {line}"

            name, officialName, capital, populationText = fields
            population = int(populationText)
            country = Country(name, officialName, capital, population)
            db.add_country(country)
    return db

def get_cli_args():
    parser = argparse.ArgumentParser()

    parser.add_argument("-m", "--module",
        metavar="MODULE", default="CountryDatabase",
        help="""The name of the module in the generated file.
            Default: CountryDatabase""")

    parser.add_argument("input_tsv",
        help="The TSV file that has the country data")

    parser.add_argument("output_gren",
        help="The Gren file that this program will create")

    return parser.parse_args()

def main():
    args = get_cli_args()
    db = build_database(args.input_tsv)
    db.write_gren_file(args.output_gren, args.module)


if __name__ == "__main__":
    main()
